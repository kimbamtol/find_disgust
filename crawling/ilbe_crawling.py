import json
import re
import sys
import time
import logging
from pathlib import Path
from typing import Dict, List

import bs4
import requests
import certifi
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC

# â”€â”€ ë¡œê·¸ ì„¤ì • â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
logging.basicConfig(
    filename="ilbe_crawl_errors.log",
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s",
    encoding="utf-8"
)

# â”€â”€ Selenium ì„¤ì • (í—¤ë“œë¦¬ìŠ¤ ì œê±° â†’ ë¸Œë¼ìš°ì € ì°½ ë„ìš°ê¸°) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
CHROMEDRIVER = r"C:\Users\OptLab\Desktop\tori\Myë…¼ë¬¸\GECCO_2025\New_TSP_GPT_GA\chromedriver-win64\chromedriver.exe"
service = Service(CHROMEDRIVER)

opts = webdriver.ChromeOptions()
# opts.add_argument("--headless=new")  # ì£¼ì„ ì²˜ë¦¬í•˜ì—¬ ì‹¤ì œ ì°½ì´ ëœ¨ë„ë¡ í•¨
opts.add_argument("--disable-gpu")
opts.add_argument("--no-sandbox")
opts.add_argument("--ignore-certificate-errors")
opts.add_argument("--allow-insecure-localhost")
opts.add_argument("--window-size=1920,1080")

driver = webdriver.Chrome(service=service, options=opts)
wait = WebDriverWait(driver, 15)

# â”€â”€ requests ì„¸ì…˜ (certifi ë²ˆë“¤ ì‚¬ìš©) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
sess = requests.Session()
sess.headers.update({
    "User-Agent": "Mozilla/5.0 Chrome/124 Safari/537.36",
    "Referer": "https://www.ilbe.com/"
})
sess.verify = certifi.where()

# â”€â”€ parse_list_page í•¨ìˆ˜: Seleniumìœ¼ë¡œ ë¦¬ìŠ¤íŠ¸ í˜ì´ì§€ ë¡œë“œ í›„ id, url, ëŒ“ê¸€ìˆ˜ ì¶”ì¶œ â”€â”€â”€
def parse_list_page(page: int) -> List[Dict]:
    """
    Seleniumìœ¼ë¡œ ILBE ë¦¬ìŠ¤íŠ¸ í˜ì´ì§€ë¥¼ ì—´ê³ , BeautifulSoupìœ¼ë¡œ íŒŒì‹±í•˜ì—¬
    ê²Œì‹œë¬¼ id, full URL, ëŒ“ê¸€ ìˆ˜ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
    """
    list_url = f"https://www.ilbe.com/list/ilbe?page={page}&listStyle=list"
    try:
        driver.get(list_url)
        time.sleep(2)  # í˜ì´ì§€ ë¡œë”©ì„ ìœ„í•´ ì ì‹œ ëŒ€ê¸°
    except Exception as e:
        logging.error(f"[parse_list_page] Seleniumìœ¼ë¡œ ë¦¬ìŠ¤íŠ¸ í˜ì´ì§€ {page} ë¡œë“œ ì‹¤íŒ¨: {e}")
        return []

    soup = bs4.BeautifulSoup(driver.page_source, "lxml")
    posts: List[Dict] = []
    for li in soup.select("ul.board-body > li"):
        classes = li.get("class", [])
        if "notice-line" in classes or "ad-line" in classes:
            continue

        c_tag = li.select_one("span.comment a")
        t_tag = li.select_one("span.title a.subject")
        if not (c_tag and t_tag):
            continue

        try:
            comment_cnt = int(c_tag.text.strip())
        except ValueError:
            continue

        href = t_tag["href"]  # e.g. "/view/8269027950?page=20000&listStyle=list"
        m = re.search(r"/view/(\d+)", href)
        if not m:
            continue
        art_id = int(m.group(1))
        full_url = f"https://www.ilbe.com/view/{art_id}"

        posts.append({
            "id": art_id,
            "url": full_url,
            "comments": comment_cnt
        })

    return posts

# â”€â”€ scrape_post í•¨ìˆ˜: ë³¸ë¬¸ + ëŒ“ê¸€(í˜ì´ì§• í¬í•¨) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def scrape_post(url: str) -> Dict:
    """
    ì£¼ì–´ì§„ ê²Œì‹œë¬¼ URLì„ Seleniumìœ¼ë¡œ ì—´ì–´,
    â€“ ì œëª©, ì‘ì„±ì, ë‚ ì§œ, ë³¸ë¬¸ í…ìŠ¤íŠ¸, ì´ë¯¸ì§€ URL, ì¶”ì²œ/ë¹„ì¶”ì²œ ìˆ˜
    â€“ ëŒ“ê¸€ í˜ì´ì§•(â€œloadComment(1)â€ â†’ â€œloadComment(2)â€ â†’ â€¦ ìˆœì„œ)ìœ¼ë¡œ ëª¨ë‘ ìˆ˜ì§‘
    ë”•ì…”ë„ˆë¦¬ í˜•íƒœë¡œ ë°˜í™˜í•©ë‹ˆë‹¤.
    """
    driver.get(url)
    wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, "div.post-content")))

    # â€” ì œëª© â€”
    try:
        title = driver.find_element(By.CSS_SELECTOR, "meta[property='og:title']").get_attribute("content")
    except:
        title = driver.title

    # â€” ì‘ì„±ì + IP â€”
    nick_raw = driver.find_element(By.CSS_SELECTOR, "span.nick").text.strip()
    ip_m = re.search(r"\((.*?)\)", nick_raw)

    # â€” ë‚ ì§œ â€”
    date = driver.find_element(By.CSS_SELECTOR, "span.date").text.strip()

    # â€” ë³¸ë¬¸ í…ìŠ¤íŠ¸ + ì´ë¯¸ì§€ ëª©ë¡ â€”
    content_div = driver.find_element(By.CSS_SELECTOR, "div.post-content")
    text_parts = [
        p.text.strip()
        for p in content_div.find_elements(By.CSS_SELECTOR, "p")
        if p.text.strip()
    ]
    img_tags = content_div.find_elements(By.CSS_SELECTOR, "img")
    images = [
        img.get_attribute("src")
        for img in img_tags
        if img.get_attribute("src")
    ]

    # â€” ì¶”ì²œ/ë¹„ì¶”ì²œ ì¹´ìš´íŠ¸ í—¬í¼ â€”
    def cnt(sel: str) -> int:
        els = driver.find_elements(By.CSS_SELECTOR, sel)
        if not els:
            return 0
        txt = els[0].text.replace(",", "").strip()
        try:
            return int(txt)
        except ValueError:
            return 0

    # â”€â”€ ëŒ“ê¸€ ìˆ˜ì§‘: â€œloadComment(1)â€ â†’ â€œloadComment(2)â€ â†’ â€¦ ìˆœì„œ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    def selenium_fetch_comments(page_delay: float = 0.5) -> List[Dict]:
        """
        í˜„ì¬ ëŒ“ê¸€ í˜ì´ì§• ì˜ì—­ì— ë³´ì´ëŠ” ëª¨ë“  í˜ì´ì§€ ë²ˆí˜¸ë¥¼ í™•ì¸í•œ ë’¤,
        1ë¶€í„° ìµœëŒ€ í˜ì´ì§€ ë²ˆí˜¸ê¹Œì§€ ìˆœì„œëŒ€ë¡œ loadComment(n)ì„ í˜¸ì¶œí•˜ì—¬ ì¶”ì¶œí•©ë‹ˆë‹¤.
        """
        def extract() -> List[Dict]:
            wait.until(EC.presence_of_all_elements_located((By.CSS_SELECTOR, "div.comment-item-box")))
            out: List[Dict] = []
            for itm in driver.find_elements(By.CSS_SELECTOR, "div.comment-item"):
                try:
                    author = itm.find_element(By.CSS_SELECTOR, "span.global-nick.nick a").text.strip()
                    date_c = itm.find_element(By.CSS_SELECTOR, "span.date-line").text.strip()
                    text_c = itm.find_element(By.CSS_SELECTOR, "span.cmt").text.strip()
                    good_e = itm.find_elements(By.CSS_SELECTOR, "em[id^='cnt_good_']")
                    bad_e = itm.find_elements(By.CSS_SELECTOR, "em[id^='cnt_bad_']")
                    good = good_e[0].text.strip() if good_e else "0"
                    bad = bad_e[0].text.strip() if bad_e else "0"
                    out.append({
                        "author": author,
                        "date": date_c,
                        "content": text_c,
                        "likes": good,
                        "dislikes": bad,
                        "llm_hate_speech": None,
                        "llm_misogyny": None,
                        "Keyword": None,
                        "keyword_content": None
                    })
                except Exception:
                    continue
            return out

        comments_list: List[Dict] = []

        # (1) í˜„ì¬ í˜ì´ì§€ì˜ ìµœëŒ€ ëŒ“ê¸€ í˜ì´ì§€ ë²ˆí˜¸ íŒŒì•…
        try:
            # ëª¨ë“  í˜ì´ì§€ ë²„íŠ¼ <a onclick="loadComment(n)"> ìš”ì†Œ ìˆ˜ì§‘
            page_btns = driver.find_elements(By.CSS_SELECTOR, "div.paginate a")
            page_nums = []
            for btn in page_btns:
                txt = btn.text.strip()
                if txt.isdigit():
                    try:
                        page_nums.append(int(txt))
                    except ValueError:
                        continue
            max_page = max(page_nums) if page_nums else 1
        except Exception:
            max_page = 1

        # (2) 1ë¶€í„° max_pageê¹Œì§€ ìˆœì„œëŒ€ë¡œ loadComment(n) í˜¸ì¶œí•˜ë©° ëŒ“ê¸€ ìˆ˜ì§‘
        for p in range(1, max_page + 1):
            try:
                # JavaScriptë¡œ ì§ì ‘ loadComment(p) í˜¸ì¶œ
                driver.execute_script(f"loadComment({p});")
                time.sleep(page_delay)
                comments_list.extend(extract())
            except Exception:
                # í•´ë‹¹ í˜ì´ì§€ ë¡œë“œì— ì‹¤íŒ¨í•˜ë©´ ë£¨í”„ ì¢…ë£Œ
                break

        return comments_list

    post = {
        "title": title,
        "url": url,
        "writer": nick_raw.split("(")[0].strip(),
        "writer_ip": ip_m.group(1) if ip_m else "â€”",
        "date": date,
        "content_text": "\n".join(text_parts),
        "content_images": images,
        "likes": cnt("span.recomm-vote > em, span.recomm"),
        "dislikes": cnt("span.recomm-vote.bad > em, span.non-recomm"),
        "comments": selenium_fetch_comments(),
        "llm_hate_speech": None,
        "llm_misogyny": None,
        "Keyword": None,
        "keyword_content": None
    }
    return post

# â”€â”€ crawl í•¨ìˆ˜: í‰ê·  ëŒ“ê¸€ìˆ˜ ì´ìƒ ê²Œì‹œë¬¼ë§Œ í¬ë¡¤ë§ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def crawl(start_page: int, end_page: int = 1):
    """
    ë¦¬ìŠ¤íŠ¸ í˜ì´ì§€ start_pageë¶€í„° end_pageê¹Œì§€(ë‚´ë¦¼ì°¨ìˆœ) ìˆœíšŒí•˜ë©°:
      1) parse_list_page (Selenium)ë¡œ (id, url, comments) ëª©ë¡ ê°€ì ¸ì˜¤ê¸°
      2) ëŒ“ê¸€ ìˆ˜ í‰ê·  ê³„ì‚°
      3) í‰ê·  ì´ìƒ ê²Œì‹œë¬¼ë§Œ scrape_postë¡œ í¬ë¡¤ë§ í›„ JSON ì €ì¥
    """
    Path("ilbe_result").mkdir(exist_ok=True)

    for page in range(start_page, end_page - 1, -1):
        print(f"\nğŸ“„ ë¦¬ìŠ¤íŠ¸ í˜ì´ì§€ {page} í¬ë¡¤ë§â€¦")
        posts_meta = parse_list_page(page)
        if not posts_meta:
            print("  â””â”€ ê¸€ ì—†ìŒ/ìš”ì²­ ì‹¤íŒ¨")
            continue

        # ëŒ“ê¸€ ìˆ˜ í‰ê·  ê³„ì‚°
        comment_counts = [p["comments"] for p in posts_meta]
        avg_comments = sum(comment_counts) / len(comment_counts)
        print(f"  Â· ì´ í˜ì´ì§€ í‰ê·  ëŒ“ê¸€ìˆ˜: {avg_comments:.2f}")

        # í‰ê·  ì´ìƒ ê²Œì‹œë¬¼ë§Œ í•„í„°ë§
        filtered = [p for p in posts_meta if p["comments"] >= avg_comments]
        print(f"  Â· í‰ê·  ì´ìƒ ê²Œì‹œë¬¼ ê°œìˆ˜: {len(filtered)} / {len(posts_meta)}")

        for meta in filtered:
            art_id = meta["id"]
            print(f"  [{art_id}] {meta['url']} (ëŒ“ê¸€ìˆ˜={meta['comments']})", end=" ")
            try:
                post_data = scrape_post(meta["url"])
                n_com = len(post_data["comments"])
                if n_com == 0:
                    logging.warning(f"[{art_id}] ëŒ“ê¸€ 0ê°œ (URL: {meta['url']})")

                # JSONìœ¼ë¡œ ì €ì¥
                Path(f"ilbe_result/{art_id}.json").write_text(
                    json.dumps({"ilbe_meta": meta, "post": post_data},
                               ensure_ascii=False, indent=2),
                    "utf-8-sig"
                )
                print(f"â†’ ì €ì¥ âœ“ (ì‹¤ì œ ìˆ˜ì§‘ ëŒ“ê¸€ {n_com}ê°œ)")
            except Exception as e:
                logging.exception(f"[{art_id}] í¬ë¡¤ë§ ì‹¤íŒ¨")
                print("ERROR:", e)

            time.sleep(1)

# â”€â”€ ì—”íŠ¸ë¦¬í¬ì¸íŠ¸ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
if __name__ == "__main__":
    argc = len(sys.argv)
    if argc == 3:
        sp, ep = int(sys.argv[1]), int(sys.argv[2])
    elif argc == 2:
        sp, ep = int(sys.argv[1]), 1
    else:
        sp = int(input("ì‹œì‘ ë¦¬ìŠ¤íŠ¸ í˜ì´ì§€â‰« ").strip())
        ep = 1

    crawl(sp, ep)
    driver.quit()
