import json
import re
import sys
import time
import logging
from pathlib import Path
from typing import Dict, List

import bs4
import requests
import certifi
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC

# â”€â”€ ë¡œê·¸ ì„¤ì • â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
logging.basicConfig(
    filename="crawl_errors.log",
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s",
    encoding="utf-8"
)

# â”€â”€ Selenium ì„¤ì • (í—¤ë“œë¦¬ìŠ¤ ì˜µì…˜ ì œê±° â†’ ë¸Œë¼ìš°ì € ì°½ ë„ìš°ê¸°) â”€â”€â”€â”€â”€â”€â”€â”€â”€
CHROMEDRIVER = r"C:\Users\OptLab\Desktop\tori\Myë…¼ë¬¸\GECCO_2025\New_TSP_GPT_GA\chromedriver-win64\chromedriver.exe"
service = Service(CHROMEDRIVER)

opts = webdriver.ChromeOptions()
# headless ëª¨ë“œë¥¼ ì œê±°í•´ì„œ ì‹¤ì œ ì°½ì´ ëœ¨ë„ë¡ í•©ë‹ˆë‹¤.
# opts.add_argument("--headless=new")
opts.add_argument("--disable-gpu")
opts.add_argument("--no-sandbox")
opts.add_argument("--ignore-certificate-errors")
opts.add_argument("--allow-insecure-localhost")
opts.add_argument("--window-size=1920,1080")

driver = webdriver.Chrome(service=service, options=opts)
wait = WebDriverWait(driver, 15)

# â”€â”€ requests ì„¸ì…˜ (certifië¡œ SSL ê²€ì¦) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
sess = requests.Session()
sess.headers.update({
    "User-Agent": "Mozilla/5.0 Chrome/124 Safari/537.36",
    "Referer": "https://www.ilbe.com/"
})
sess.verify = certifi.where()

# â”€â”€ parse_list_page í•¨ìˆ˜: Seleniumìœ¼ë¡œ ë¦¬ìŠ¤íŠ¸ í˜ì´ì§€ ë¡œë“œ í›„ id, url, ëŒ“ê¸€ìˆ˜ ì¶”ì¶œ â”€â”€â”€
def parse_list_page(page: int) -> List[Dict]:
    """
    Seleniumì„ ì‚¬ìš©í•˜ì—¬ ILBE ë¦¬ìŠ¤íŠ¸ í˜ì´ì§€ë¥¼ ì§ì ‘ ì—´ê³ , BeautifulSoupìœ¼ë¡œ íŒŒì‹±í•©ë‹ˆë‹¤.
    ê° ê²Œì‹œë¬¼ì˜ id, url, comments(ëŒ“ê¸€ ìˆ˜)ë¥¼ ì¶”ì¶œí•œ ë’¤ ë¦¬ìŠ¤íŠ¸ë¡œ ë°˜í™˜í•©ë‹ˆë‹¤.
    """
    list_url = f"https://www.ilbe.com/list/ilbe?page={page}&listStyle=list"
    try:
        driver.get(list_url)
        # í˜ì´ì§€ ë¡œë”©ì´ ì™„ë£Œë  ë•Œê¹Œì§€ ì¶©ë¶„íˆ ê¸°ë‹¤ë¦½ë‹ˆë‹¤.
        # (ì—¬ê¸°ì„œëŠ” ì„ì˜ë¡œ 2ì´ˆ ì •ë„ sleepì„ ë‘ì—ˆì§€ë§Œ, í•„ìš”í•˜ë©´ ì ì ˆíˆ ì¡°ì ˆí•˜ì„¸ìš”)
        time.sleep(2)
        # ë˜ëŠ” ì•„ë˜ì²˜ëŸ¼ íŠ¹ì • ìš”ì†Œê°€ ë¡œë“œë  ë•Œê¹Œì§€ ëª…ì‹œì ìœ¼ë¡œ ê¸°ë‹¤ë¦´ ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤.
        # wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, "ul.board-body > li")))
    except Exception as e:
        logging.error(f"[parse_list_page] Seleniumìœ¼ë¡œ ë¦¬ìŠ¤íŠ¸ í˜ì´ì§€ {page} ë¡œë“œ ì‹¤íŒ¨: {e}")
        return []

    # ë¸Œë¼ìš°ì €ê°€ ë Œë”ë§í•œ HTML ì „ì²´ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.
    page_source = driver.page_source
    soup = bs4.BeautifulSoup(page_source, "lxml")

    posts: List[Dict] = []
    for li in soup.select("ul.board-body > li"):
        classes = li.get("class", [])
        if "notice-line" in classes or "ad-line" in classes:
            continue

        # ëŒ“ê¸€ ìˆ˜ <span class="comment"><a>ìˆ«ì</a></span>
        c_tag = li.select_one("span.comment a")
        # ì œëª©+ë§í¬ <span class="title"><a class="subject" href="...">...</a>...</span>
        t_tag = li.select_one("span.title a.subject")

        if not (c_tag and t_tag):
            continue

        # ëŒ“ê¸€ ìˆ˜ë¥¼ ì •ìˆ˜ë¡œ ë³€í™˜
        try:
            comment_cnt = int(c_tag.text.strip())
        except ValueError:
            continue

        # hrefì—ì„œ ID ì¶”ì¶œ
        href = t_tag["href"]  # e.g. "/view/8269027950?page=20000&listStyle=list"
        m = re.search(r"/view/(\d+)", href)
        if not m:
            continue
        art_id = int(m.group(1))
        full_url = f"https://www.ilbe.com/view/{art_id}"

        posts.append({
            "id": art_id,
            "url": full_url,
            "comments": comment_cnt
        })

    return posts

# â”€â”€ scrape_post í•¨ìˆ˜: ë³¸ë¬¸+ëŒ“ê¸€(í˜ì´ì§• í¬í•¨) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def scrape_post(url: str) -> Dict:
    """
    ì£¼ì–´ì§„ ê²Œì‹œë¬¼ URLì„ Seleniumìœ¼ë¡œ ì—´ì–´,
    â€“ ì œëª©, ì‘ì„±ì, ì‘ì„±ì¼, ë³¸ë¬¸ í…ìŠ¤íŠ¸, ì´ë¯¸ì§€ URL, ì¶”ì²œ/ë¹„ì¶”ì²œ ìˆ˜
    â€“ ëŒ“ê¸€ í˜ì´ì§•(ì²«í˜ì´ì§€ â†’ ë‹¤ìŒ â†’ ë‹¤ìŒ ...) ë°©ì‹ìœ¼ë¡œ ì „ë¶€ ìˆ˜ì§‘
    ê²°ê³¼ë¥¼ ë”•ì…”ë„ˆë¦¬ë¡œ ë°˜í™˜í•©ë‹ˆë‹¤.
    """
    driver.get(url)
    wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, "div.post-content")))

    # â€” ì œëª© â€”
    try:
        title = driver.find_element(By.CSS_SELECTOR, "meta[property='og:title']").get_attribute("content")
    except:
        title = driver.title

    # â€” ì‘ì„±ì + IP â€”
    nick_raw = driver.find_element(By.CSS_SELECTOR, "span.nick").text.strip()
    ip_m = re.search(r"\((.*?)\)", nick_raw)

    # â€” ë‚ ì§œ â€”
    date = driver.find_element(By.CSS_SELECTOR, "span.date").text.strip()

    # â€” ë³¸ë¬¸ í…ìŠ¤íŠ¸ + ì´ë¯¸ì§€ ëª©ë¡ â€”
    content_div = driver.find_element(By.CSS_SELECTOR, "div.post-content")
    text_parts = [
        p.text.strip()
        for p in content_div.find_elements(By.CSS_SELECTOR, "p")
        if p.text.strip()
    ]
    img_tags = content_div.find_elements(By.CSS_SELECTOR, "img")
    images = [
        img.get_attribute("src")
        for img in img_tags
        if img.get_attribute("src")
    ]

    # â€” ì¶”ì²œ/ë¹„ì¶”ì²œ ì¹´ìš´íŠ¸ í—¬í¼ â€”
    def cnt(sel: str) -> int:
        els = driver.find_elements(By.CSS_SELECTOR, sel)
        if not els:
            return 0
        txt = els[0].text.replace(",", "").strip()
        try:
            return int(txt)
        except ValueError:
            return 0

    # â”€â”€ ëŒ“ê¸€ ìˆ˜ì§‘: â€œì²«í˜ì´ì§€ â†’ ë‹¤ìŒ â†’ ë‹¤ìŒ â€¦â€ ìˆœì„œ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    def selenium_fetch_comments(page_delay: float = 0.5) -> List[Dict]:
        """
        ëŒ“ê¸€ í˜ì´ì§• ì˜ì—­ì„ ëª¨ë‘ ìˆœíšŒí•˜ë©° comment-itemì„ ìˆ˜ì§‘í•©ë‹ˆë‹¤.
        """
        def extract() -> List[Dict]:
            wait.until(EC.presence_of_all_elements_located((By.CSS_SELECTOR, "div.comment-item-box")))
            out: List[Dict] = []
            for itm in driver.find_elements(By.CSS_SELECTOR, "div.comment-item"):
                try:
                    author = itm.find_element(By.CSS_SELECTOR, "span.global-nick.nick a").text.strip()
                    date_c = itm.find_element(By.CSS_SELECTOR, "span.date-line").text.strip()
                    text_c = itm.find_element(By.CSS_SELECTOR, "span.cmt").text.strip()
                    good_e = itm.find_elements(By.CSS_SELECTOR, "em[id^='cnt_good_']")
                    bad_e = itm.find_elements(By.CSS_SELECTOR, "em[id^='cnt_bad_']")
                    good = good_e[0].text.strip() if good_e else "0"
                    bad = bad_e[0].text.strip() if bad_e else "0"
                    out.append({
                        "author": author,
                        "date": date_c,
                        "content": text_c,
                        "likes": good,
                        "dislikes": bad,
                        "llm_hate_speech": None,
                        "llm_misogyny": None,
                        "Keyword": None,
                        "keyword_content": None
                    })
                except Exception:
                    continue
            return out

        comments_list: List[Dict] = []

        # (a) â€œì²«í˜ì´ì§€â€ ë²„íŠ¼ í´ë¦­
        try:
            first_btn = wait.until(
                EC.element_to_be_clickable((
                    By.XPATH,
                    "//div[@class='paginate']/a[contains(@class,'prev') and normalize-space(text())='ì²«í˜ì´ì§€']"
                ))
            )
            first_btn.click()
            time.sleep(page_delay)
        except Exception:
            # ëŒ“ê¸€ì´ í•œ í˜ì´ì§€ë°–ì— ì—†ìœ¼ë©´ ì´ ë¶€ë¶„ì—ì„œ ì˜ˆì™¸ê°€ ë‚˜ê³ , ë°”ë¡œ extract()ë§Œ ìˆ˜í–‰
            comments_list.extend(extract())
            return comments_list

        # (b) 1í˜ì´ì§€ ëŒ“ê¸€ ìˆ˜ì§‘
        comments_list.extend(extract())

        # (c) â€œë‹¤ìŒâ€ ë²„íŠ¼ ë°˜ë³µ í´ë¦­ â†’ 2,3,4... ëª¨ë‘ ìˆ˜ì§‘
        while True:
            try:
                nxt_btn = wait.until(EC.element_to_be_clickable((By.CSS_SELECTOR, "div.paginate a.next2")))
            except Exception:
                break

            if "disabled" in (nxt_btn.get_attribute("class") or "").lower():
                break

            try:
                prev_active = driver.find_element(By.CSS_SELECTOR, "div.paginate a.page-on")
                prev_page_no = int(prev_active.text.strip())
            except Exception:
                prev_page_no = None

            nxt_btn.click()
            time.sleep(page_delay)

            try:
                new_active = driver.find_element(By.CSS_SELECTOR, "div.paginate a.page-on")
                new_page_no = int(new_active.text.strip())
            except Exception:
                new_page_no = None

            if prev_page_no is not None and new_page_no == prev_page_no:
                break

            comments_list.extend(extract())

        return comments_list

    post = {
        "title": title,
        "url": url,
        "writer": nick_raw.split("(")[0].strip(),
        "writer_ip": ip_m.group(1) if ip_m else "â€”",
        "date": date,
        "content_text": "\n".join(text_parts),
        "content_images": images,
        "likes": cnt("span.recomm-vote > em, span.recomm"),
        "dislikes": cnt("span.recomm-vote.bad > em, span.non-recomm"),
        "comments": selenium_fetch_comments(),
        "llm_hate_speech": None,
        "llm_misogyny": None,
        "Keyword": None,
        "keyword_content": None
    }
    return post

# â”€â”€ crawl í•¨ìˆ˜: í‰ê·  ëŒ“ê¸€ìˆ˜ ì´ìƒ ê²Œì‹œë¬¼ë§Œ í¬ë¡¤ë§ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def crawl(start_page: int, end_page: int = 1):
    """
    ë¦¬ìŠ¤íŠ¸ í˜ì´ì§€ start_pageë¶€í„° end_pageê¹Œì§€(ë‚´ë¦¼ì°¨ìˆœ) ìˆœíšŒí•˜ë©°:
      1) parse_list_page (Selenium) ë¡œ (id, url, comments) ëª©ë¡ ê°€ì ¸ì˜¤ê¸°
      2) ëŒ“ê¸€ ìˆ˜ í‰ê·  ê³„ì‚°
      3) í‰ê·  ì´ìƒ ê²Œì‹œë¬¼ë§Œ scrape_post ë¡œ í¬ë¡¤ë§í•´ì„œ JSON ì €ì¥
    """
    Path("result").mkdir(exist_ok=True)

    for page in range(start_page, end_page - 1, -1):
        print(f"\nğŸ“„ ë¦¬ìŠ¤íŠ¸ í˜ì´ì§€ {page} í¬ë¡¤ë§â€¦")
        posts_meta = parse_list_page(page)
        if not posts_meta:
            print("  â””â”€ ê¸€ ì—†ìŒ/ìš”ì²­ ì‹¤íŒ¨")
            continue

        # ëŒ“ê¸€ ìˆ˜ í‰ê·  ê³„ì‚°
        comment_counts = [p["comments"] for p in posts_meta]
        avg_comments = sum(comment_counts) / len(comment_counts)
        print(f"  Â· ì´ í˜ì´ì§€ í‰ê·  ëŒ“ê¸€ìˆ˜: {avg_comments:.2f}")

        # í‰ê·  ì´ìƒ ê²Œì‹œë¬¼ë§Œ í•„í„°ë§
        filtered = [p for p in posts_meta if p["comments"] >= avg_comments]
        print(f"  Â· í‰ê·  ì´ìƒ ê²Œì‹œë¬¼ ê°œìˆ˜: {len(filtered)} / {len(posts_meta)}")

        for meta in filtered:
            art_id = meta["id"]
            print(f"  [{art_id}] {meta['url']} (ëŒ“ê¸€ìˆ˜={meta['comments']})", end=" ")

            try:
                post_data = scrape_post(meta["url"])
                n_com = len(post_data["comments"])
                if n_com == 0:
                    logging.warning(f"[{art_id}] ëŒ“ê¸€ 0ê°œ (URL: {meta['url']})")

                # JSONìœ¼ë¡œ ì €ì¥
                Path(f"result/{art_id}.json").write_text(
                    json.dumps({"ilbe_meta": meta, "post": post_data},
                               ensure_ascii=False, indent=2),
                    "utf-8-sig"
                )
                print(f"â†’ ì €ì¥ âœ“ (ì‹¤ì œ ìˆ˜ì§‘ ëŒ“ê¸€ {n_com}ê°œ)")
            except Exception as e:
                logging.exception(f"[{art_id}] í¬ë¡¤ë§ ì‹¤íŒ¨")
                print("ERROR:", e)

            time.sleep(1)

# â”€â”€ ì—”íŠ¸ë¦¬í¬ì¸íŠ¸ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
if __name__ == "__main__":
    argc = len(sys.argv)
    if argc == 3:
        sp, ep = int(sys.argv[1]), int(sys.argv[2])
    elif argc == 2:
        sp, ep = int(sys.argv[1]), 1
    else:
        sp = int(input("ì‹œì‘ ë¦¬ìŠ¤íŠ¸ í˜ì´ì§€â‰« ").strip())
        ep = 1

    crawl(sp, ep)
    driver.quit()
